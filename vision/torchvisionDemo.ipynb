{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from easydict import EasyDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=EasyDict()\n",
    "params.path=\"/home/hzc/workshop/gitfile/data/cifar10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgshow(img):\n",
    "    img= img\n",
    "    imgn = img.numpy()\n",
    "    plt.imshow(np.transpose(imgn,(1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgTransforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root=params.path,train=True,transform=imgTransforms,download=False)\n",
    "testset = torchvision.datasets.CIFAR10(root=params.path,train=False,transform=transforms,download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(trainset[1][0].shape)\n",
    "imgshow(torchvision.utils.make_grid(trainset[2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(trainset,batch_size=32,shuffle=True)\n",
    "testLoader = torch.utils.data.DataLoader(testset,batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=5*5*16, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "ecoch 0, batch   100, loss: 230.502\n",
      "ecoch 0, batch   200, loss: 230.388\n",
      "ecoch 0, batch   300, loss: 230.280\n",
      "ecoch 0, batch   400, loss: 230.337\n",
      "ecoch 0, batch   500, loss: 230.182\n",
      "ecoch 0, batch   600, loss: 230.124\n",
      "ecoch 0, batch   700, loss: 230.130\n",
      "ecoch 0, batch   800, loss: 230.120\n",
      "ecoch 0, batch   900, loss: 230.006\n",
      "ecoch 0, batch  1000, loss: 229.934\n",
      "ecoch 0, batch  1100, loss: 229.820\n",
      "ecoch 0, batch  1200, loss: 229.647\n",
      "ecoch 0, batch  1300, loss: 229.377\n",
      "ecoch 0, batch  1400, loss: 229.043\n",
      "ecoch 0, batch  1500, loss: 228.759\n",
      "epoch: 1\n",
      "ecoch 1, batch   100, loss: 226.734\n",
      "ecoch 1, batch   200, loss: 224.804\n",
      "ecoch 1, batch   300, loss: 220.114\n",
      "ecoch 1, batch   400, loss: 216.218\n",
      "ecoch 1, batch   500, loss: 210.108\n",
      "ecoch 1, batch   600, loss: 208.293\n",
      "ecoch 1, batch   700, loss: 200.742\n",
      "ecoch 1, batch   800, loss: 196.822\n",
      "ecoch 1, batch   900, loss: 196.067\n",
      "ecoch 1, batch  1000, loss: 194.297\n",
      "ecoch 1, batch  1100, loss: 189.027\n",
      "ecoch 1, batch  1200, loss: 188.480\n",
      "ecoch 1, batch  1300, loss: 186.617\n",
      "ecoch 1, batch  1400, loss: 181.631\n",
      "ecoch 1, batch  1500, loss: 178.637\n",
      "epoch: 2\n",
      "ecoch 2, batch   100, loss: 177.344\n",
      "ecoch 2, batch   200, loss: 174.617\n",
      "ecoch 2, batch   300, loss: 174.645\n",
      "ecoch 2, batch   400, loss: 174.313\n",
      "ecoch 2, batch   500, loss: 168.738\n",
      "ecoch 2, batch   600, loss: 170.722\n",
      "ecoch 2, batch   700, loss: 166.340\n",
      "ecoch 2, batch   800, loss: 162.764\n",
      "ecoch 2, batch   900, loss: 162.094\n",
      "ecoch 2, batch  1000, loss: 161.133\n",
      "ecoch 2, batch  1100, loss: 159.386\n",
      "ecoch 2, batch  1200, loss: 157.627\n",
      "ecoch 2, batch  1300, loss: 160.399\n",
      "ecoch 2, batch  1400, loss: 159.212\n",
      "ecoch 2, batch  1500, loss: 157.869\n",
      "Train done!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    print(\"epoch:\",epoch)\n",
    "    lossOfNet = 0.0\n",
    "    for i,data in enumerate(trainLoader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        output = net(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lossOfNet+=loss.item()\n",
    "        if i%100==99:\n",
    "            print(\"ecoch %d, batch %5d, loss: %.3f\"%(epoch, i+1, lossOfNet))\n",
    "            lossOfNet=0.0\n",
    "print(\"Train done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataiter = iter(testLoader) who it can not work?\n",
    "dataiter = iter(trainLoader)\n",
    "correct=0\n",
    "total=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in dataiter:\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted==labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4394\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\",correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
